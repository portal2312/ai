# Generative AI and language models

생성형 AI와 언어모델의 이해.

학습 목표:

- 식별형 AI와 생성형 AI의 차이와 특징을 파악할 수 있다.
- 인공 지능이 어떤 구조로 인간의 언어를 이해하고 처리하는 지 설명할 수 있다.

## Discriminative AI and Generative AI

식별형 AI 와 생성형 AI.

예:

- 강아지 사진 > Discriminative AI > 강아지
- 강아지 > Generative AI > 다양한 강아지 사진

### Discriminative AI

식별형 AI.

일련의 기능을 기반으로 데이터를 사전에 정리된 클래스로 분류하도록 훈련된 인공지능.

입력된 데이터를 기반으로 예측하도록 훈련된다.

예: 자율 주행(길거리에 사람과 사물을 인식)

### Generative AI

생성형 AI.

기본적인 확률 분포를 기반으로 새로운 데이터 샘플을 만들어 내도록 훈련된 인공지능.

훈련 데이터와 유사한 새로운 데이터를 생성하도록 훈련된다.

예: 화풍을 학습하여 새로운 그림 그리기.

## 인공지능이 인간의 언어를 이해하는 법

### Cleaver Hans Effect

똑똑한 말, 한스.

모든 정확한 의미를 파악하지 못하더라도 맥락, 분위기를 알고 답을 맞출 수 가 있다.

사람이 이해하는 방식으로 인지하지 못하더라도 인공지능은 우리가 필요로 하는 언어를 만들 수 있다. 한스 처럼 비슷하게.

언어라는 것을 이해하고 만들어 낼 때, 사람이 하는 방식 그대로는 아니더라도 또 다른 의미의 지능은 분명한 것이고 이러한 것이 구현되어 있는 것이 언어 모델이다.

### Language models

언어 모델.

언어 모델을 만드는 방법을 통계적 기법과 인공 신경망으로 구분한다.

- 과거는 통계적 기법이 많이 활용 되었으나, 점차 인공 신경망 기반의 언어 모델(예: ChatGPT)이 증가하고 있다.
- 2017년 구글의 "Attention is all you need" 논문에서 Transformer 를 발표하면서 자연어 처리 분야를 평정했다.
- 자연어 처리는 Transformer 기반으로 BEAT(), OpenAI 두 모델을 중심으로 발전하고 있다.

| 구분      | BERT                                                                                                                                                  | GPT                                                                                                             |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| 개발사    | Google                                                                                                                                                | OpenAI                                                                                                          |
| 학습 방향 | - 양방향                                                                                                                                              | - 단방향                                                                                                        |
|           | - 입력된 문장의 모든 단어를 이용해서 학습하므로 문장 전체를 이해하는 능력이 높다.                                                                     | - 입력된 문장을 왼쪽부터 오른쪽으로 한 단어씩 입력해 모델을 학습하므로 문맥에 맞는 단어를 생성하는 능력이 높다. |
| 언어 생성 | Encoder 와 Decoder 모두 사용한다. 입력문을 Encoder 로 임베딩 후 Decoder 로 출력한다.                                                                  | Decoder 만 사용한다. 단어를 순서대로 학습해 출력문을 생성하는 방식이다.                                         |
| 예시      | "나는 학교에 갔다" 를 "나는", "학교에", "갔다" 로 개별 단어로 보지 않고 "나는(주어) 학교에(위치) 갔다(행동)" 으로 전체를 이해한 후 출력문을 생성한다. | "나는 학교에" 를 문맥상 다음에 올 단어가 "먹었다" 또는 "입었다" 가 아닌 "갔다" 를 예측하여 출력문을 생성한다.   |

언어 모델은 문장(단어 시퀀스)에 확률을 할당해 가장 자연스러운 문장(단어 시퀀스)을 찾아내는 방법이다.

- ChatGPT: 앞 단어가 주어지면 뒷 단어를 예측하는 방식으로 동작한다: "나는 소설을 _쓴다_"
- BERT: 앞 뒤의 단어의 주어진 단어로부터 가운데 빈 단어를 예측하는, 일종의 빈 칸 추론 문제풀이 방식이다: "아이들이 _하하_ 웃으며 달려 간다"

> [!NOTE]
> ChatGPT 를 사용하다 보면 정말 사람의 언어을 이해하는 느낌을 받을 수 있습니다.
> 엄밀히 말하면, 이는 Cleaver Hans Effect 입니다.
> 만약, 주변인들이 잘못 된 답을 알고 있다면 Hans 는 주변인들로부터 잘못 된 답을 알려 주었을 것입니다.
> 여러분이 어떠한 사람인가 따라 답이 달라 질 수 있습니다.
> 인공지능을 다룸에 있어 우리의 능력이 그만큼 따라가야되고 훌륭한 주인이 되어야 합니다.
